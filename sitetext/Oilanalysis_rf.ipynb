{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil analysis using random forest classification \n",
    "The essentials of a random forest are best illustrated by introducing its decision trees first. A decision tree shows how well a partitioning of the data predicts a label that has been assigned to the data. For example, the demo-script below is geared to distinguish used oil from fresh oil. The picture shows a random forest of two decision trees that predict the label \"green\" by partitioning a training set of measurements ($x$,$y$). The picture illustrates that various decision trees may be selected and that the choice of the decision tree determines the prediction of the label.\n",
    "\n",
    "![image](figures/Oilanalysis_rf01.png).\n",
    "\n",
    "Clearly, the decision tree at the lefthand side is better as it is shorter and it labels the measurements ($x$,$y$) perfectly. The decision tree at the righthand side is larger and it only imperfectly labels the measurements. A node in a decision tree should ideally yield pure subsets. The CART algorithm (Classification And Regression Tree) uses the Gini impurity to quantify the diversity of the labels within a set by:\n",
    "\n",
    "$I=\\sum_{i=1}^{n} p_i \\times{(1-p_i)}$\n",
    "\n",
    "where $i$ is the number of labels in a set, i.e. (ocher, green) and $p_i$ is the proportion of these labels in the set. The impurity of the set of all measurements ($x$,$y$) is:\n",
    "\n",
    "$I_{u}= p_{ocher} \\times{(1-p_{ocher})}+p_{green} \\times{(1-p_{green})}=2/5 \\times{3/5} +3/5 \\times{2/5}=12/25 $\n",
    " \n",
    "The impurity of the three sets generated by the decision tree at the righthand side is:\n",
    "\n",
    "$I_{\\ge{y} \\;\\;}= p_{ocher} \\times{(1-p_{ocher})}+p_{green} \\times{(1-p_{green})}=1/2 \\times{1/2}+1/2 \\times{1/2}=1/2$\n",
    "\n",
    "$I_{\\lt{y}\\ge{x}}= p_{ocher} \\times{(1-p_{ocher})}+p_{green} \\times{(1-p_{green})}=0 \\times{1}+1 \\times{0}=0$\n",
    "\n",
    "$I_{\\lt{y}\\lt{x}}= p_{ocher} \\times{(1-p_{ocher})}+p_{green} \\times{(1-p_{green})}=1 \\times{0}+0 \\times{1}=0 $\n",
    "\n",
    "The Gini gain $G$ of the impurity of the unpartitioned measurements $I_{u}$ times the $p_{j}$ weighted impurity of the $j$ partitioned measurements $I_{j}$:\n",
    "\n",
    "$G=I_{u}-\\sum_{j=1}^{m} I_{j} \\times{p_{j}}$\n",
    "\n",
    "The Gini gain of the decision tree at the righthand side follows from the Gini impurities calculated above and the sizes of these sets:\n",
    "\n",
    "$G=12/25-(1/2 \\times{2/5}+ 0 \\times{2/5}+ 0 \\times{1/5})=7/25$\n",
    "\n",
    "Similarly, it can be shown that the Gini gain of the decision tree at the lefthand side equals $12/25$ as the decision tree yields data sets with impurity $0$. The Gini gain of the decision tree at the righthand side exceeds the Gini gain at the lefthand side which provides a criterion to compare the decision trees in a random forest.\n",
    "\n",
    "### ... is insensitive to many scaling and transformations\n",
    "An ...\n",
    "\n",
    "\n",
    "Here, \n",
    "\n",
    "### ... is insensitive to irrelevant data\n",
    "Dependencies\n",
    " \n",
    "\n",
    "### ... is sensitive to the composition of the training set\n",
    "..\n",
    "\n",
    "# [Click here to see the random forest script](https://nbviewer.jupyter.org/github/chrisrijsdijk/RAMS/blob/master/notebook/Oilanalysis_randomforest.ipynb?flush_cache=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
